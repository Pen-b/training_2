{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd429803-03a3-48fd-b630-881cf6327c59",
   "metadata": {},
   "source": [
    "# Rule based Subject-Verb-Object\n",
    "\n",
    "Methods from this [StackOverflow](https://stackoverflow.com/questions/39763091/how-to-extract-subjects-in-a-sentence-and-their-respective-dependent-phrases) answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95322e7-cf1e-470e-9f81-389b63236184",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts =  [\n",
    "    # https://www.bbc.co.uk/news/world-us-canada-60177979\n",
    "    (\n",
    "        \"The US East Coast is hunkering down as a major blizzard hits the region for the first time in four years.\"\n",
    "        \"The storm is forecast to stretch from the Carolinas to Maine, packing hurricane-force winds in coastal parts.\"\n",
    "        \"Five states have declared emergencies.\"\n",
    "        \"Mayor Michelle Wu of Boston, a city that is no stranger to snowfall, said the storm could be 'historic'.\"\n",
    "        \"More than two feet of snow could fall in New England.\"\n",
    "        \"Weather officials also warn of flooding near the coast.\"\n",
    "        \"Over 5,000 US flights were cancelled between Friday and Sunday, according to FlightAware.\"\n",
    "        \"Forecasters say there is a chance the storm, known as a Nor\\'easter, will blanket the Boston area with up to 2ft (61cm) of snow.\"\n",
    "        ),\n",
    "    # https://www.bbc.co.uk/news/business-60163814\n",
    "    (  \n",
    "        \"Apple sales soared in the key Christmas shopping season, despite constraints due to a global shortage of microchips.\"\n",
    "        \"Sales at the iPhone giant rose 11% to a record $123.9bn (Â£92.6bn) in the October to December period, beating forecasts.\"\n",
    "        \"Shares jumped more than 4% in after-hours trade, as the report suggested the firm's pandemic boom is continuing.\"\n",
    "        \"Apple has seen purchases skyrocket during the pandemic as people spend more time online.\"\n",
    "        \"The firm's market value briefly hit the $3tn milestone in early January though its share price has slipped more recently amid weeks of market turmoil.\"\n",
    "        ),\n",
    "    # https://news.sky.com/story/staycation-frenzy-spurs-center-parcs-owner-to-prepare-4bn-sale-12527982\n",
    "    (\n",
    "        \"Sky News has learnt that Brookfield Property Partners, the Canadian property giant, is paving the way to sell Center Parcs UK potentially as soon as this year.\"\n",
    "        \"City sources said this weekend that Brookfield had engaged the accountancy firm PriceWaterhouseCoopers to assist with preparations for a sale process.\"\n",
    "        \"Investment banks have yet to be formally appointed to handle an auction, and one person close to the process said it was possible that Brookfield would decide to retain the business for a longer period if it did not secure a sufficiently attractive offer.\"\n",
    "        \"Center Parcs is one of the most famous brands in the British leisure industry, drawing millions of visitors annually to its five UK sites and the latest addition to its portfolio, at Longford Forest in Ireland.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec83183-db1a-4eb5-8d5d-d1cdafabd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "ADJECTIVES = [\"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\",\n",
    "              \"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\",\" possessive\"]\n",
    "COMPOUNDS = [\"compound\"]\n",
    "PREPOSITIONS = [\"prep\"]\n",
    "\n",
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "\n",
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs\n",
    "\n",
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs\n",
    "\n",
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False\n",
    "\n",
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def findSVs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs\n",
    "\n",
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
    "    return objs\n",
    "\n",
    "def getAdjectives(toks):\n",
    "    toks_with_adjectives = []\n",
    "    for tok in toks:\n",
    "        adjs = [left for left in tok.lefts if left.dep_ in ADJECTIVES]\n",
    "        adjs.append(tok)\n",
    "        adjs.extend([right for right in tok.rights if tok.dep_ in ADJECTIVES])\n",
    "        tok_with_adj = \" \".join([adj.lower_ for adj in adjs])\n",
    "        toks_with_adjectives.extend(adjs)\n",
    "\n",
    "    return toks_with_adjectives\n",
    "\n",
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated\n",
    "\n",
    "def getAllObjs(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "def getAllObjsWithAdjectives(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "\n",
    "    if len(objs)== 0:\n",
    "        objs = [tok for tok in rights if tok.dep_ in ADJECTIVES]\n",
    "\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n",
    "    return svos\n",
    "\n",
    "def findSVAOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjsWithAdjectives(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    obj_desc_tokens = generate_left_right_adjectives(obj)\n",
    "                    sub_compound = generate_sub_compound(sub)\n",
    "                    svos.append((\" \".join(tok.lower_ for tok in sub_compound), \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, \" \".join(tok.lower_ for tok in obj_desc_tokens)))\n",
    "    return svos\n",
    "\n",
    "def generate_sub_compound(sub):\n",
    "    sub_compunds = []\n",
    "    for tok in sub.lefts:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            sub_compunds.extend(generate_sub_compound(tok))\n",
    "    sub_compunds.append(sub)\n",
    "    for tok in sub.rights:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            sub_compunds.extend(generate_sub_compound(tok))\n",
    "    return sub_compunds\n",
    "\n",
    "def generate_left_right_adjectives(obj):\n",
    "    obj_desc_tokens = []\n",
    "    for tok in obj.lefts:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "    obj_desc_tokens.append(obj)\n",
    "\n",
    "    for tok in obj.rights:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "\n",
    "    return obj_desc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9b3798-93dd-4a6d-bbbf-cd53aa811e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c050a4a-d73a-4148-a5e5-1cb11188a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfb376a-478e-4a5f-9686-a78b6e2a781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0896aa29-5d63-4de7-87c5-c4846aa8ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "svos = []\n",
    "verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "for v in verbs:\n",
    "    subs, verbNegated = getAllSubs(v)\n",
    "    v, objs = getAllObjsWithAdjectives(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bdbe57b-a03a-4bb8-bb85-8b45eb8e4962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([storm], blanket, [area])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs,v,objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f472c34-ad4e-46b6-a293-44a133e6ad0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([area], [storm])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_left_right_adjectives(objs[0]),generate_sub_compound(subs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "080f8c24-71dc-42ee-885f-d38b8bd93bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('us east coast', 'hunkering', 'hits'),\n",
       " ('blizzard', 'hits', 'region'),\n",
       " ('storm', 'forecast', 'stretch packing'),\n",
       " ('states', 'declared', 'emergencies'),\n",
       " ('city', '!is', 'stranger'),\n",
       " ('mayor michelle wu', 'said', 'be'),\n",
       " ('forecasters', 'say', 'is'),\n",
       " ('there', 'is', 'chance'),\n",
       " ('storm', 'blanket', 'area')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findSVAOs(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898b3bd-2483-48bf-a19b-8cd1dc045546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
