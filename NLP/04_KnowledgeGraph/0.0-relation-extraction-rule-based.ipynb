{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graphs: Rule Based Relation-Extraction\n",
    "\n",
    "This article demonstrates how to construct a knowledge graph using rule-based methods.\n",
    "\n",
    "Knowledge Graphs (KG's) are increasingly the focus of machine-learning and data-mining research efforts toward human-level computational cognition and intelligence. Subsequently, knowledge graphs have become critical tools in many data heavy fields such as biomedical research and drug discovery, materials science, recommendation systems, intelligence gathering and political risk among others.\n",
    "\n",
    "- https://www.sciencedirect.com/science/article/pii/S0957417420305881\n",
    "- https://arxiv.org/abs/2002.00388\n",
    "\n",
    "## What is a knowledge graph? \n",
    "\n",
    "Within the [ATI knowledge-graph working group](https://www.turing.ac.uk/research/interest-groups/knowledge-graphs) we use the following definition *\"Knowledge graphs organise data from multiple sources, capture information about entities of interest in a given domain or task (like people, places or events), and forge connections between them.\"*\n",
    "\n",
    "If you have ever been tasked to tried to take notes and to map out complex information - perhaps in a workshop or brainstorm - you might have ended up with a web of network of notes. Similar idea.\n",
    "\n",
    "KG's are data stored in graph form. When you think about a data model for knowldege, you can break it down and organise it into:\n",
    "1. The \"things\" or entities of interest - represented as nodes. \n",
    "1. What are the attributes and relationships between these things - represented as edges. \n",
    "\n",
    "As an example, consider this diagram about the some the main characters, things and how they relate from The Mandolorian tv show.\n",
    "\n",
    "<img src=\"./kg-example.png\" width=600 height=400>\n",
    "\n",
    "\n",
    "## How are knowledge graphs constructed?\n",
    "\n",
    "Text-mining and Natural Language Processing (NLP) techniques are used to process raw text into a knowledge graph structure (see below). A typical pipeline will involve:\n",
    "\n",
    "1.  **Entity Extraction:** The \"things\" and entities of interest are identified and captured from within the text data.\n",
    "\n",
    "1. **Coreference Resolution:** The task of finding all mentions of the same entity within a text. For instance, if the entity \"Luke Skywalker\" is later referenced by a personal pronoun \"He is a Jedi\".\n",
    "\n",
    "1. **Relationship Extraction:** This step identifies what kind of relationship, if any, exists between any two entities.\n",
    "\n",
    "Each step can be achieved using rule-based methods (e.g. Regex patterns), and or machine-learning methods (e.g. Named Entity Recognition). \n",
    "\n",
    "<img src=\"./nlp-pipeline.png\" width=600 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85600930b325b2bc99c604b7535cf298e475c247"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts =  [\n",
    "    # https://www.bbc.co.uk/news/world-us-canada-60177979\n",
    "    (\n",
    "        \"The US East Coast is hunkering down as a major blizzard hits the region for the first time in four years.\"\n",
    "        \"The storm is forecast to stretch from the Carolinas to Maine, packing hurricane-force winds in coastal parts.\"\n",
    "        \"Five states have declared emergencies.\"\n",
    "        \"Mayor Michelle Wu of Boston, a city that is no stranger to snowfall, said the storm could be 'historic'.\"\n",
    "        \"More than two feet of snow could fall in New England.\"\n",
    "        \"Weather officials also warn of flooding near the coast.\"\n",
    "        \"Over 5,000 US flights were cancelled between Friday and Sunday, according to FlightAware.\"\n",
    "        \"Forecasters say there is a chance the storm, known as a Nor\\'easter, will blanket the Boston area with up to 2ft (61cm) of snow.\"\n",
    "        ),\n",
    "    # https://www.bbc.co.uk/news/business-60163814\n",
    "    (  \n",
    "        \"Apple sales soared in the key Christmas shopping season, despite constraints due to a global shortage of microchips.\"\n",
    "        \"Sales at the iPhone giant rose 11% to a record $123.9bn (Â£92.6bn) in the October to December period, beating forecasts.\"\n",
    "        \"Shares jumped more than 4% in after-hours trade, as the report suggested the firm's pandemic boom is continuing.\"\n",
    "        \"Apple has seen purchases skyrocket during the pandemic as people spend more time online.\"\n",
    "        \"The firm's market value briefly hit the $3tn milestone in early January though its share price has slipped more recently amid weeks of market turmoil.\"\n",
    "        ),\n",
    "    # https://news.sky.com/story/staycation-frenzy-spurs-center-parcs-owner-to-prepare-4bn-sale-12527982\n",
    "    (\n",
    "        \"Sky News has learnt that Brookfield Property Partners, the Canadian property giant, is paving the way to sell Center Parcs UK potentially as soon as this year.\"\n",
    "        \"City sources said this weekend that Brookfield had engaged the accountancy firm PriceWaterhouseCoopers to assist with preparations for a sale process.\"\n",
    "        \"Investment banks have yet to be formally appointed to handle an auction, and one person close to the process said it was possible that Brookfield would decide to retain the business for a longer period if it did not secure a sufficiently attractive offer.\"\n",
    "        \"Center Parcs is one of the most famous brands in the British leisure industry, drawing millions of visitors annually to its five UK sites and the latest addition to its portfolio, at Longford Forest in Ireland.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2099872afd539fa28784a066f2ce0c06c42a2585"
   },
   "source": [
    "##  Part-Of-Speech (POS) tags\n",
    "\n",
    "In linguistics and grammar, A part of speech or part-of-speech (POS) is the category of a word that have similar grammatical properties. \n",
    "For instance \"nouns\" are words for real things like people, places and objects. Words that describe nouns are called \"adjectives\" such as: tall, smart, large. \n",
    "\n",
    "Applications in Natural Language Processing (NLP) apply linguistic rules and machine learning models to predict and assign which POS tags apply by evaluating word position and context. Popular NLP packages such as NLTK and spaCy include this functionality OOTB. To read more about POS, see this [POS summary](https://towardsdatascience.com/part-of-speech-tagging-for-beginners-3a0754b2ebba), the spaCy [documentation](https://spacy.io/usage/linguistic-features#pos-tagging) and [SO explanation](https://stackoverflow.com/questions/40288323/what-do-spacys-part-of-speech-and-dependency-tags-mean), and this [POS tag reference list](https://sites.google.com/site/partofspeechhelp/#TOC-Welcome).\n",
    "\n",
    "> **NOTE:**\n",
    "spaCy uses the terms *head* and *child* to describe the words connected by a single arc in the dependency tree. The term *dep* is used for the arc label, which describes the type of syntactic relation that connects the child to the head.\n",
    "\n",
    "We can build a basic relation-extraction process by using grammar patterns / part of speech patterns to identify related nouns within a text. A simple rule might be:\n",
    "\n",
    "```\n",
    "Proper Noun - Verb - Proper Noun\n",
    "```\n",
    "\n",
    "Using spaCy, we can now iterate over each sentence and identify where this POS pattern occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd80efd1e651449ee42847f5859feadea27a15ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nouns = ['NNP','NN','NNS']\n",
    "verbs = [\"VBZ\",\"VB\",\"VBG\"]\n",
    "relations = list()\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    for e,sent in enumerate(doc.sents):\n",
    "        chain = list()\n",
    "        for a in sent:\n",
    "            if a.tag_ in nouns: # find first NOUND\n",
    "                chain.append(a)\n",
    "                for b in sent[a.i:]: # find ROOT, alternatively VERBS\n",
    "                    if (b.dep_ == 'ROOT') and len(chain) == 1:\n",
    "                        chain.append(b)\n",
    "                        for c in sent[b.i:]: # find second NOUN\n",
    "                            if c.tag_ in nouns and len(chain) == 2: \n",
    "                                chain.append(c)\n",
    "                                \n",
    "                                # reset chain and print result\n",
    "                                relations.append(chain)\n",
    "                                pos_chain = ' '.join([f\"{i} ({i.tag_}|{i.dep_})\" for i in sent[a.i:c.i+1]])\n",
    "                                print(chain,'\\n',pos_chain,'\\n')\n",
    "                                chain = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this simple approach seems OK at finding sentences that contain related entities.\n",
    "\n",
    "- US --- **hunkering** ---> Blizzard\n",
    "- Sales --- **soared** ---> Christmas\n",
    "- Sky --- **learnt** ---> Brookfield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction\n",
    "\n",
    "The problem with the above approach is that it relies on an extensive list of *Part-Of-Speech* tag patterns. This won't scale for most problems as nouns and verbs come in a wide variety of forms and with modifiers etc. For instance, you generally want to capture compound term phrases and patterns such as:\n",
    "```\n",
    "Metro-North worker = NNP-HYPH-NNP\n",
    "Killed by = VBZ-IN\n",
    "```\n",
    "\n",
    "To improve our method we can:\n",
    " 1. Better capture and relfect things and objects - collectively named \"entities\".\n",
    "     \n",
    " \n",
    " 1. Develop POS patterns and rules to identify and extract relations between two or more entities. \n",
    "\n",
    " 1. Train a probabilistic model to identify relation triplets such as [Stanford, OLLIE - see reddit]\n",
    " \n",
    "### Capture entities\n",
    "\n",
    "A spacy pipeline with components for Named Entity Recognition (NER) and Noun Chunks is used to capture entities. Here, we could define some sensible rules or limit the number and type of entities to control what information will be represented in our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "    nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))\n",
    "    print(nlp.pipe_names)\n",
    "except:\n",
    "    print(nlp.pipe_names)\n",
    "\n",
    "\n",
    "doc = nlp(texts[2])\n",
    "print('\\n', ' '.join([f\"{d} ({d.tag_}|{d.dep_})\" for d in doc]),'\\n')\n",
    "spacy.displacy.render(doc, style='ent')\n",
    "\n",
    "print('Entities:')\n",
    "for t in doc:\n",
    "    if t.ent_type_ != '': print('\\t',t,t.ent_type_)\n",
    "\n",
    "print('Noun chunks:')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print('\\t',chunk.text, )\n",
    "    #chunk.root.text, chunk.root.dep_,chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture relations\n",
    "\n",
    "For each sentence, relations can be extracted by iterating through the entity pairs and noun chunks, and yielding the ROOT VERB and other VERB terms. Spacy's dependency parser operates on each sentence in isolation and so it is not possible to extract relations across sentences with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(span,ent_types=None,noun_chunks=False):\n",
    "    entities = dict()\n",
    "    if ent_types is None:\n",
    "        ent_types = ['ORG','PERSON','GPE','NORP','LOC','PRODUCT']\n",
    "        \n",
    "    if noun_chunks:\n",
    "        entities.update({int(f\"{i.start}{i.end}\"): {\"span\":i, \"type\":\"NOUN_CHUNK\"} for i in span.noun_chunks})\n",
    "        \n",
    "    entities.update({int(f\"{i.start}{i.end}\"): {\"span\":i, \"type\":i.label_} for i in span.ents if i.label_ in ent_types})\n",
    "    keys = sorted(entities)\n",
    "    return entities, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents: \n",
    "    s_term_pos = \" \".join([f\"{t} ({t.tag_}|{t.dep_})\" for t in sent])\n",
    "    entities, keys = get_entities(sent)\n",
    "\n",
    "    if len(keys) > 1:\n",
    "        pairs = [(x,y) for x,y in zip(keys,keys[1:])]\n",
    "        print('\\n',s_term_pos)\n",
    "        # print(keys,pairs)\n",
    "        for p in pairs:\n",
    "            start,end = entities[p[0]], entities[p[1]]\n",
    "            for w in doc[start['span'].start:end['span'].end]:\n",
    "                if w.tag_ in ['VBZ','VBN','VBG','VBD','VB']:\n",
    "                    print(f\"\\t>>>\\t\",f\"{start['span']} - {w}({w.tag_}|{w.dep_}) - {end['span']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.displacy.render(doc, style='ent')\n",
    "# spacy.displacy.render(sent, style='dep',)\n",
    "# spacy.explain('attr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better - but not great. There's a few additional rules we could add to expand and improve relationship capture. \n",
    "\n",
    "- capture direct ascendants and decedents https://spacy.io/usage/linguistic-features#navigating-around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direct_relations(sent,entities):\n",
    "    ent_text = [v['span'].text for v in entities.values()]\n",
    "    relations = list()\n",
    "\n",
    "    for v in entities.values():\n",
    "        \n",
    "        if v['span'].n_lefts > 0:\n",
    "            lf = [i for i in v['span'].lefts if i.text in ent_text]\n",
    "            if len(lf) > 0: \n",
    "                relations.append((lf[0].text,'=IS',v['span'].text,'DIRECT'))\n",
    "        if v['span'].n_rights > 0:\n",
    "            rt = [i for i in v['span'].rights if i.text in ent_text]\n",
    "            if len(rt) > 0: \n",
    "                relations.append((v['span'].text,'=IS',rt[0].text,'DIRECT'))\n",
    "\n",
    "    if len(relations) > 0:\n",
    "        return relations\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in list(doc.sents)[:]: \n",
    "    s_term_pos = \" \".join([f\"{t} ({t.tag_}|{t.dep_})\" for t in sent])\n",
    "    entities, keys = get_entities(sent, noun_chunks=True)\n",
    "    \n",
    "    sentence_relations = get_direct_relations(sent,entities)\n",
    "    if sentence_relations is not None:\n",
    "        relations+=sentence_relations\n",
    "        \n",
    "        print(s_term_pos)\n",
    "        for r in sentence_relations:\n",
    "            print('\\t',r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get relationships between the subject and object term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ent_pos_relations(doc, sentence_entities):\n",
    "    \"\"\"\n",
    "    Entity to POS relations\n",
    "    \"\"\"\n",
    "    SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "    OBJECTS = [\"pobj\",\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "    SUB_OBJ = SUBJECTS+OBJECTS\n",
    "    \n",
    "    feature = [  \"ROOT\",\n",
    "                 \"aux\",\"relcl\",\n",
    "                 \"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\",\n",
    "                 \"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\",\" possessive\",\n",
    "                 \"compound\",\n",
    "                 \"prep\"]\n",
    "\n",
    "    relations = list()\n",
    "    for e in sentence_entities.values():\n",
    "        prev = list()\n",
    "        for t in doc[e['span'].end:sent.end]:\n",
    "            if t.dep_ in ['pobj','nsubj','dojb']: \n",
    "                terms = doc[e['span'].end:t.i]\n",
    "                rels = [t for t in terms if t.dep_ in feature if t.idx not in prev]\n",
    "                prev.extend([x.idx for x in rels]) # previous terms\n",
    "                \n",
    "                if len(rels) > 0:\n",
    "                    rels = ' '.join([x.text for x in rels])\n",
    "                    relations.append((e['span'].text,rels,t.text,'ENT_POS'))\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in list(doc.sents): \n",
    "    s_term_pos = \" \".join([f\"{t} ({t.tag_}|{t.dep_})\" for t in sent])\n",
    "    entities, keys = get_entities(sent, noun_chunks=True)\n",
    "    \n",
    "    sentence_relations = get_ent_pos_relations(doc,entities)\n",
    "    if sentence_relations is not None:\n",
    "        relations+=sentence_relations\n",
    "\n",
    "        print(s_term_pos)\n",
    "        for r in sentence_relations:\n",
    "            print('\\t',r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the relations in an \"edge list\" using e.g. an dataframe object or database table for further inspection and pruning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relationships = pd.DataFrame(relations,columns=['name1','relation','name2','relation_type'])\n",
    "entity_map = {x:e for e,x in enumerate(pd.unique(entity_relationships[['name1','name2']].values.ravel()))}\n",
    "entity_relationships['source_id'] = entity_relationships['name1'].map(entity_map)\n",
    "entity_relationships['target_id'] = entity_relationships['name2'].map(entity_map)\n",
    "entity_relationships[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Knowledge Graph\n",
    "\n",
    "Now we are ready to build our knowledge graph. As the name implies a suitable structure to represent the data is in a [graph or complex network](https://en.wikipedia.org/wiki/Complex_network#Definition) data structure. Complex networks are suited to model non-trivial topological data structures and enable rich analysis of, among others, structural patterns, connectivity, cycles, hierarchy, shortest path, page rank, clustering, small world and randomness, etc. \n",
    "\n",
    "In mathematics, graph are commonly denoted $G(v,e)$ of $v$ vertices or nodes linked by $e$ edges. Our knowledge graph may then be $KG(e,r)$ of $e$ entities and noun chunks, and their relations $r$.\n",
    "\n",
    "In our case, it is important that we construct the knowledge graph using a directed graph. This is because the ordering and direction of the relationships we have extracted is important. For instance \"Center Park's\" is **IN** \"Ireland\" and not \n",
    "*vis versa*.\n",
    "\n",
    "There may be relationships types that do relate in each direction, or insights that can be gained by simply looking at entity connectivity and the number of relationships, or even link prediction. Complex networks, graph data structures, and graph analysis provide a wealth of options to analyse these data. \n",
    "\n",
    "> If you want to learn more about complex networks I recommend the websites, books, and learning materials of: [Prof Ernesto Estrada](https://sites.google.com/view/ernestoestrada/home), [Prof Albert Barabasi](https://www.barabasilab.com/), and [SNAP the Stanford Network Analysis Project](https://snap.stanford.edu/index.html).\n",
    "\n",
    "Now, we create a directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create graph\n",
    "G=nx.DiGraph(name='simple KG')\n",
    "\n",
    "# add nodes\n",
    "for k,v in entity_map.items():\n",
    "    G.add_node(v,title=k,label=k, ent_typ='node') \n",
    "\n",
    "# add edges\n",
    "edges = [(row['source_id'],\n",
    "          row['target_id'],\n",
    "          {'weight':1, \n",
    "           'relation':row['relation'],\n",
    "           'title':row['relation']}) for ix,row in entity_relationships.iterrows()]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# add a size param\n",
    "sd = {k[0]:1+(k[1]**2) for k in G.degree}\n",
    "nx.set_node_attributes(G, sd, \"size\")\n",
    "\n",
    "# save\n",
    "joblib.dump(G,'graph.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Knowledge Graph of Entity Relationships\n",
    "\n",
    "One of the powerful applications of knowledge graph is search and information discovery. User interfaces and visualisations can be used to navigate the graph to discover patterns and relationships in the data.\n",
    "\n",
    "\n",
    "#### Networkx\n",
    "\n",
    "First, there is [Networkx](https://networkx.org/documentation/stable/reference/drawing.html?highlight=layout#) for quick graph analytics and visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PlotGraph:\n",
    "    \n",
    "    def __init__(self,):\n",
    "        pass\n",
    "\n",
    "    def node_partition_color(self,graph):\n",
    "        \"\"\"\n",
    "        compute the node partitions for colouring\n",
    "        \"\"\"\n",
    "\n",
    "        if graph.is_directed():\n",
    "            # for directed graphs\n",
    "            c = list(greedy_modularity_communities(graph))\n",
    "            node_color=dict()\n",
    "            for e,s in enumerate(c):\n",
    "                for n in s:\n",
    "                    node_color[n] = e\n",
    "            node_color = [node_color[k] for k in sorted(node_color.keys())]\n",
    "\n",
    "        else:\n",
    "            # for un-directed graphs\n",
    "            partition = community.best_partition(graph)\n",
    "            node_color= partition\n",
    "\n",
    "        return node_color\n",
    "    \n",
    "    def plot_graph(self,axis,graph):\n",
    "        node_size = [15+(v**3) for k,v in graph.degree]\n",
    "        node_labels = {node[0]:node[1]['title'] for node in graph.nodes(data=True)}\n",
    "        edge_labels = {(i[0],i[1]):i[2]['relation'] for i in graph.edges(data=True)}\n",
    "        pos = nx.layout.spring_layout(graph,iterations=300,k=4)\n",
    "        \n",
    "        nx.draw(graph,pos=pos,\n",
    "                ax=axis,\n",
    "                node_color=self.node_partition_color(graph),\n",
    "                node_size=node_size,\n",
    "                labels=node_labels,\n",
    "                width=0.9)\n",
    "        \n",
    "        nx.draw_networkx_edge_labels(graph,pos,\n",
    "                                     ax=axis,\n",
    "                                     edge_labels=edge_labels,\n",
    "                                     font_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph\n",
    "fig,axs = plt.subplots(1,1, figsize=(15,15))\n",
    "pg = PlotGraph()\n",
    "pg.plot_graph(axs,G)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have added a function to colour the node's within the tightly connected \"modular\" portions of the network. When plotting the entire graph this highlights some distinct connected components and subgraphs of the overall knowledge graph. I.e. there are some entities and nodes that are not connected to one another at all. \n",
    "\n",
    "This is not surprising given the nature of the input data. Now, some of the content and entities in this data are for \"Sky News\" and about the expansion of \"Center Park's\".\n",
    "\n",
    "I could go an retrieve some more text about these entities, process them for relations, and add them to the graph. Would that be helpful? Possibly. But it would also create and relations from the new texts and that might be on new topics such as covid booking numbers or the appointment of a new CEO. Anything. The point is, when constructing the knowledge graph it is important to consider what data you want to model otherwise you might end up with a web of spaghetti.\n",
    "\n",
    "Below, I split the graphs by connected component and re-plot. Note that I converted the DirectedGraph to an un-directed Graph to calculate the components as I'm just interested in any relationship at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uG = G.to_undirected()\n",
    "sg = [G.subgraph(c) for c in nx.connected_components(uG)]\n",
    "ng = len(sg)\n",
    "\n",
    "# plot graph\n",
    "fig,axs = plt.subplots(ng,1, figsize=(9,ng*9))\n",
    "pg = PlotGraph()\n",
    "for ax,g in zip(axs.flatten(),sg):\n",
    "    pg.plot_graph(ax,g)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [PyVis](https://pyvis.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "\n",
    "PyVis provides interactive physics and in-line notebook network plotting. Unfortunately, it seems some customisation of the css would be required to display the edge names in our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "net = Network(notebook=True,directed=True,height=\"700px\",width=\"100%\")\n",
    "net.from_nx(sg[1])\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.show('graph.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streamlit\n",
    "\n",
    "Lastly, there is the [Streamlit Agraph component](https://github.com/ChrisDelClea/streamlit-agraph).\n",
    "\n",
    "<img src=\"./streamlit.png\" width=600 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "## References\n",
    " - [information-extraction-with-dominating-rules](https://github.com/philipperemy/information-extraction-with-dominating-rules)\n",
    " - [Pruning Knowledge Graphs](http://philipperemy.github.io/information-extract/)\n",
    "\n",
    " - [Knowledge Graph â A Powerful Data Science Technique to Mine Information from Text](https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/)\n",
    " - [Spacy subject-object extraction](https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py)\n",
    "     -  [subject-object dependency parsing](https://stackoverflow.com/questions/39763091/how-to-extract-subjects-in-a-sentence-and-their-respective-dependent-phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
