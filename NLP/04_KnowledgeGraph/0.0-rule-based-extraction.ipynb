{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b9c1dde0b2464ff6778d75d84efd3ba2ef21848"
   },
   "source": [
    "# Rule based methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "85600930b325b2bc99c604b7535cf298e475c247"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample data set and apply Spacy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_title</th>\n",
       "      <th>named_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Apple, Unveils, Souped, Up, MacBook, Pro, wit...</td>\n",
       "      <td>((Apple), (MacBook, Pro, with, Retina))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Delays, To, Intel, 's, Broadwell, CPUs, Bounc...</td>\n",
       "      <td>((Intel), (Broadwell))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Apple, Cuts, Mac, Book, Pro, Price, by, Rs, 1...</td>\n",
       "      <td>((Apple), (11000), (India))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         spacy_title  \\\n",
       "0  (Apple, Unveils, Souped, Up, MacBook, Pro, wit...   \n",
       "1  (Delays, To, Intel, 's, Broadwell, CPUs, Bounc...   \n",
       "2  (Apple, Cuts, Mac, Book, Pro, Price, by, Rs, 1...   \n",
       "\n",
       "                            named_entities  \n",
       "0  ((Apple), (MacBook, Pro, with, Retina))  \n",
       "1                   ((Intel), (Broadwell))  \n",
       "2              ((Apple), (11000), (India))  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/uci-news-aggregator\"\n",
    "files = os.listdir(path)\n",
    "df = pd.read_csv(os.path.join(path,files[1]),nrows=500)\n",
    "\n",
    "\n",
    "# Apply spacy to the article titles\n",
    "df[\"spacy_title\"] = df[\"TITLE\"].apply(lambda x : nlp(x))\n",
    "\n",
    "# add field of NE\n",
    "df[\"named_entities\"] = df[\"spacy_title\"].apply(lambda x : x.ents)\n",
    "\n",
    "df[['spacy_title','named_entities']][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2099872afd539fa28784a066f2ce0c06c42a2585"
   },
   "source": [
    "##  Part-Of-Speech (POS) pattern recognition\n",
    "[here is a list of POS](https://sites.google.com/site/partofspeechhelp/home/nnp_nnps#TOC-Definition-of-NNPS-Proper-Noun-Plural-Form-1)\n",
    "\n",
    "Now, we will define a grammar pattern / part of speech pattern to identify what type of relations we want to extract from the data. \n",
    "\n",
    "Let's we are interested in finding an action relation between two named entities. so we can define a pattern using part of speech tags as : \n",
    "\n",
    "Proper Noun - Verb - Proper Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_chain_1 = \"NNP-VBZ-NNP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98847a394b96a3abfc71e8d9ea28ed77f27d4b0a"
   },
   "source": [
    "Using spaCy, we can now iterate in text and identify what are the relevant triplets (governer, relation, dependent) or in other terms, what are the entities and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Apple Cuts Mac Book Pro Price by Rs 11000 in India,\n",
       " 'NNP-VBZ-NNP-NNP-NNP-NNP-IN-NNS-CD-IN-NNP',\n",
       " 'NNP-VBZ-NNP')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(df.loc[2,'TITLE'])\n",
    "pos_chain = \"-\".join([d.tag_ for d in doc])\n",
    "doc,pos_chain,pos_chain_1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 11)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "  \n",
    "# Find start end\n",
    "def substr_index(string,pattern):\n",
    "    a = [(m.start(),m.end()) for m in re.finditer('{0}'.format(pattern), string)]\n",
    "    return a\n",
    "\n",
    "substr_index(pos_chain,pos_chain_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple Cuts Mac Book Pro Price by Rs 11000 in India'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "fd80efd1e651449ee42847f5859feadea27a15ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Cuts Mac Book Pro Price by Rs 11000 in India\n",
      "NNP-VBZ-NNP-NNP-NNP-NNP-IN-NNS-CD-IN-NNP\n",
      "\n",
      "Apple gives MacBook Pro with Retina more power, more memory\n",
      "NNP-VBZ-NNP-NNP-IN-NN-JJR-NN-,-JJR-NN\n",
      "\n",
      "Apple refreshes MacBook Pro with Retina display lineup, drops prices\n",
      "NNP-VBZ-NNP-NNP-IN-NN-NN-NN-,-VBZ-NNS\n",
      "\n",
      "Apple refreshes MacBook Pro with Retina display line, adds mighty processor  ...\n",
      "NNP-VBZ-NNP-NNP-IN-NN-NN-NN-,-VBZ-JJ-NN-_SP-.\n",
      "\n",
      "Apple refreshes MacBook Pro with Retina display line, improved processors and  ...\n",
      "NNP-VBZ-NNP-NNP-IN-NN-NN-NN-,-JJ-NNS-CC-_SP-.\n",
      "\n",
      "Apple updates MacBook Pro Retina range with i7 processors\n",
      "NNP-VBZ-NNP-JJ-NN-NN-IN-NN-NNS\n",
      "\n",
      "Apple gives Retina MacBook Pros a speed boost ahead of Yosemite rollout\n",
      "NNP-VBZ-NNP-NNP-NNS-DT-NN-NN-RB-IN-NNP-NN\n",
      "\n",
      "Apple refreshes MacBook Pro with Retina display lineup with faster CPUs, more  ...\n",
      "NNP-VBZ-NNP-NNP-IN-NN-NN-NN-IN-JJR-NNS-,-RBR-_SP-.\n",
      "\n",
      "Apple updates Macbook Pro with Retina display line with more memory, faster  ...\n",
      "NNP-VBZ-NNP-NNP-IN-NN-NN-NN-IN-JJR-NN-,-RBR-_SP-.\n",
      "\n",
      "Apple To Buy Swell For $30 Million: Apple Acquires Pandora-Like App For  ...\n",
      "NNP-TO-VB-NN-IN-$-CD-CD-:-NNP-VBZ-NNP-HYPH-NNP-VBZ-IN-_SP-.\n",
      "\n",
      "Apple Acquires Swell for $30M for ITunes with Samsung on Hiring Silicon Spree  ...\n",
      "NNP-VBZ-NNP-IN-$-CD-NN-IN-NNP-IN-NNP-IN-VBG-NNP-NNP-_SP-.\n",
      "\n",
      "Apple Confirms BookLamp Buy, Swell App Said to Be Next\n",
      "NNP-VBZ-NNP-NNP-,-NNP-VBZ-VBD-TO-VB-JJ\n",
      "\n",
      "FDA Approves EYLEA Injection for the Treatment of Diabetic Macular Edema\n",
      "NNP-VBZ-NNP-NNP-IN-DT-NNP-IN-NNP-NNP-NNP\n",
      "\n",
      "FDA approves EYLEA Injection for treatment of Diabetic Macular Edema\n",
      "NNP-VBZ-NNP-NNP-IN-NN-IN-NNP-NNP-NNP\n",
      "\n",
      "FDA approves Regeneron's Eylea for DME treatment (REGN)\n",
      "NNP-VBZ-NNP-POS-NNP-IN-NNP-NN--LRB--NNP--RRB-\n",
      "\n",
      "Trustees Report Reveals Medicare is Solvent Beyond 2013 Projections\n",
      "NNP-NNP-VBZ-NNP-VBZ-NNP-IN-CD-NNS\n",
      "\n",
      "OVERNIGHT HEALTHCARE: HHS touts O-Care drug cost savings\n",
      "NNP-NNP-:-NNP-VBZ-NNP-HYPH-NN-NN-NN-NNS\n",
      "\n",
      "Green applauds Medicare Trustees' report\n",
      "NNP-VBZ-NNP-NNP-POS-NN\n",
      "\n",
      "Green applauds Medicare trustees report\n",
      "NNP-VBZ-NNP-NNS-NN\n",
      "\n",
      "Son says North Carolina-based missionary fighting Ebola in Liberia, still in  ...\n",
      "NNP-VBZ-NNP-NNP-HYPH-VBN-JJ-VBG-NNP-IN-NNP-,-RB-IN-_SP-.\n",
      "\n",
      "2000 Weather Deaths Per Year in US; Cold Is Top Culprit\n",
      "CD-NN-NNS-IN-NN-IN-NNP-:-NNP-VBZ-NNP-NNP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index_list = list()\n",
    "for i, r in df.iterrows():\n",
    "    pos_chain = \"-\".join([d.tag_ for d in r['spacy_title']])\n",
    "    if pos_chain_1 in pos_chain:\n",
    "        if len(r[\"named_entities\"]) >= 2:\n",
    "            index_list.append(i)\n",
    "            print (r[\"TITLE\"])\n",
    "            print (pos_chain+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from these examples, one can see different entities and relations for example: \n",
    "\n",
    "- Honda --- **restructures** ---> US operations  \n",
    "- Carl Icahn --- **slams** ---> eBay CEO\n",
    "- Google --- **confirms** ---> Android SDK \n",
    "- GM --- **hires** ---> Lehman Brothers \n",
    "\n",
    "References : https://kgtutorial.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about relation between Named Entities? \n",
    "The problem with the above approach is that one needs to have a comprehensive list of possible *Part-Of-Speech* tags defined a priori. In reality nouns and verbs come in a wide variety of forms and with modifiers etc. \n",
    "For instance you might also want to capture: IN, eg IN-VBZ, VBZ-IN, VBZ-IN-IN, VBN-IN etc\n",
    "\n",
    "To overcome this you can:\n",
    " 1. Constrain the type and number of relaitons you wish to find, create patterns for those. \n",
    " 2. Constrain the entities on which you wish to find relaitons such as Person named entities.\n",
    " 3. Train a probabilisitc model to identify relation triplets such as [Stanford, OLLIE - see reddit]\n",
    " \n",
    "Below we will try to form relations using approach 2, between named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Unveils Souped Up MacBook Pro with Retina Display\n",
      "NNP-VBZ-VBN-RP-NN-NNP-IN-NN-NN\n",
      "(Apple, 'ORG') (Unveils, 'VBZ') (MacBook Pro with Retina, 'PRODUCT') \n",
      "\n",
      "Apple Unveils Souped Up MacBook Pro with Retina Display\n",
      "NNP-VBZ-VBN-RP-NN-NNP-IN-NN-NN\n",
      "(Apple, 'ORG') (Souped, 'VBN') (MacBook Pro with Retina, 'PRODUCT') \n",
      "\n",
      "Apple unveils minor bumps to MacBook Pro laptops\n",
      "NNP-VBZ-JJ-NNS-IN-NNP-JJ-NNS\n",
      "(Apple, 'ORG') (unveils, 'VBZ') (MacBook Pro, 'PRODUCT') \n",
      "\n",
      "Retina MacBook Pro gets Faster Processors, More RAM\n",
      "NNP-NNP-NNP-VBZ-JJR-NNS-,-JJR-NN\n",
      "(MacBook Pro, 'PRODUCT') (gets, 'VBZ') (Faster Processors, 'ORG') \n",
      "\n",
      "The updated Retina MacBook Pro you've been waiting for could launch tomorrow\n",
      "DT-VBN-NNP-NNP-NNP-PRP-VB-VBN-VBG-IN-MD-VB-NN\n",
      "(Retina MacBook Pro, 'PRODUCT') (been, 'VBN') (tomorrow, 'DATE') \n",
      "\n",
      "Apple updates entire MacBook Pro line-up\n",
      "NNP-VBZ-JJ-NNP-JJ-NN-HYPH-NN\n",
      "(Apple, 'ORG') (updates, 'VBZ') (MacBook Pro, 'PRODUCT') \n",
      "\n",
      "Apple lifts lid on new Retina MacBook Pros, spec boost confirmed\n",
      "NNP-VBZ-NN-IN-JJ-NNP-NNP-NNS-,-NN-NN-VBD\n",
      "(Apple, 'ORG') (lifts, 'VBZ') (Retina MacBook Pros, 'ORG') \n",
      "\n",
      "Apple updates MacBook Pro Retina range with i7 processors\n",
      "NNP-VBZ-NNP-JJ-NN-NN-IN-NN-NNS\n",
      "(Apple, 'ORG') (updates, 'VBZ') (MacBook Pro Retina, 'PRODUCT') \n",
      "\n",
      "Apple lifts lid on new Retina MacBook Pros, spec boost confirmed\n",
      "NNP-VBZ-NN-IN-JJ-NNP-NNP-NNS-,-NN-NN-VBD\n",
      "(Apple, 'ORG') (lifts, 'VBZ') (Retina MacBook Pros, 'ORG') \n",
      "\n",
      "Apple Store down: Upgraded MacBook Pro Retinas on the way?\n",
      "NNP-NNP-RB-:-VBN-NNP-NNP-NNS-IN-DT-NN-.\n",
      "(Apple Store, 'ORG') (Upgraded, 'VBN') (MacBook Pro Retinas, 'PRODUCT') \n",
      "\n",
      "Apple Store outage hints at possible MacBook Pro refresh\n",
      "NNP-NNP-NN-VBZ-IN-JJ-NNP-JJ-NN\n",
      "(Apple Store, 'ORG') (hints, 'VBZ') (MacBook Pro, 'PRODUCT') \n",
      "\n",
      "Leaked image suggests Apple could release upgraded MacBook Pro\n",
      "VBN-NN-VBZ-NNP-MD-VB-VBN-NNP-NNP\n",
      "(Apple, 'ORG') (upgraded, 'VBN') (MacBook Pro, 'PRODUCT') \n",
      "\n",
      "Why Swell Acquisition for $30M is Beneficial to Apple?\n",
      "WRB-NNP-NNP-IN-$-CD-NN-VBZ-JJ-IN-NNP-.\n",
      "($30M, 'MONEY') (is, 'VBZ') (Apple, 'ORG') \n",
      "\n",
      "Apple confirms it has acquired Swell Radio\n",
      "NNP-VBZ-PRP-VBZ-VBN-NNP-NNP\n",
      "(Apple, 'ORG') (confirms, 'VBZ') (Swell Radio, 'ORG') \n",
      "\n",
      "Apple confirms it has acquired Swell Radio\n",
      "NNP-VBZ-PRP-VBZ-VBN-NNP-NNP\n",
      "(Apple, 'ORG') (has, 'VBZ') (Swell Radio, 'ORG') \n",
      "\n",
      "Apple confirms it has acquired Swell Radio\n",
      "NNP-VBZ-PRP-VBZ-VBN-NNP-NNP\n",
      "(Apple, 'ORG') (acquired, 'VBN') (Swell Radio, 'ORG') \n",
      "\n",
      "Apple buys talk-radio station Swell for £17.7m\n",
      "NNP-VBZ-NN-HYPH-NN-NN-VB-IN-$-CD-NN\n",
      "(Apple, 'ORG') (buys, 'VBZ') (£17.7m, 'MONEY') \n",
      "\n",
      "Apple Confirms BookLamp Buy, Swell App Said to Be Next\n",
      "NNP-VBZ-NNP-NNP-,-NNP-VBZ-VBD-TO-VB-JJ\n",
      "(Apple, 'ORG') (Confirms, 'VBZ') (BookLamp, 'PRODUCT') \n",
      "\n",
      "Apple acquires radio and podcast app Swell for $31.8M\n",
      "NNP-VBZ-NN-CC-NN-NN-VB-IN-$-CD-NN\n",
      "(Apple, 'ORG') (acquires, 'VBZ') ($31.8M, 'MONEY') \n",
      "\n",
      "Apple Is About to Acquire Podcast App Swell for $30 Million…\n",
      "NNP-VBZ-IN-TO-VB-NNP-NNP-VB-IN-$-CD-CD-NFP\n",
      "(Apple, 'ORG') (Is, 'VBZ') ($30 Million, 'MONEY') \n",
      "\n",
      "As Apple buys Swell, here's 3 \"car tricks\" they can learn from the founding designer\n",
      "IN-NNP-VBZ-VBP-,-RB-VBZ-CD-``-NN-NNS-''-PRP-MD-VB-IN-DT-VBG-NN\n",
      "(Apple, 'ORG') (buys, 'VBZ') (3, 'CARDINAL') \n",
      "\n",
      "As Apple buys Swell, here's 3 \"car tricks\" they can learn from the founding designer\n",
      "IN-NNP-VBZ-VBP-,-RB-VBZ-CD-``-NN-NNS-''-PRP-MD-VB-IN-DT-VBG-NN\n",
      "(Apple, 'ORG') ('s, 'VBZ') (3, 'CARDINAL') \n",
      "\n",
      "Apple Looking To Acquire Swell Talk Radio And Podcast App For $30 Million\n",
      "NNP-VBG-TO-VB-VB-NN-NNP-CC-NNP-VBZ-IN-$-CD-CD\n",
      "(Apple, 'ORG') (App, 'VBZ') ($30 Million, 'MONEY') \n",
      "\n",
      "Apple Looks to Strengthen iTunes Radio with $30 Million 'Swell Radio  ...\n",
      "NNP-VBZ-TO-VB-NNP-NNP-IN-$-CD-CD-''-JJ-NN-_SP-.\n",
      "(Apple, 'ORG') (Looks, 'VBZ') ($30 Million, 'MONEY') \n",
      "\n",
      "Apple Ready to Buy Swell Talk Show App in $30M Deal\n",
      "NNP-JJ-TO-VB-NN-NNP-NNP-VBZ-IN-$-CD-NNP-NNP\n",
      "(Apple, 'ORG') (App, 'VBZ') ($30, 'MONEY') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 25\n",
    "n = 0\n",
    "for i, r in df.iterrows():\n",
    "    if len(r[\"named_entities\"]) == 2:\n",
    "        ents = r[\"named_entities\"]\n",
    "        words = r['spacy_title']\n",
    "        pos_chain = \"-\".join([d.tag_ for d in r['spacy_title']])\n",
    "        \n",
    "        # for words between each NE pair\n",
    "        for w in words[ents[0].end:ents[1].start]: \n",
    "            \n",
    "            if w.tag_ == 'VBZ': # if VERB is between 2 NE\n",
    "                n += 1\n",
    "                print(words)\n",
    "                print(pos_chain)\n",
    "                print((ents[0],ents[0].label_),\n",
    "                      (w,w.tag_),\n",
    "                      (ents[1],ents[1].label_),'\\n')\n",
    "                \n",
    "            elif w.tag_ == 'VBN': # if VERB noun is between 2 NE\n",
    "                n += 1\n",
    "                print(words)\n",
    "                print(pos_chain)\n",
    "                print((ents[0],ents[0].label_),\n",
    "                      (w,w.tag_),\n",
    "                      (ents[1],ents[1].label_),'\\n')\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        if n == limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so using NE appears better able to capture our people and organisations. However, naievely creating Triplets by extracting the verbs between Entities is not that good due to:\n",
    " - It fails on complex sentence structures. \n",
    " - It ignores other objects represented by Nouns, Propper Nouns, and Common Nouns etc. \n",
    " - Not all ENTITY types are relevant: PERSON:ORDINAL\n",
    "\n",
    "We could improve some of this by incoporating **[Noun Chunks](https://spacy.io/usage/linguistic-features#noun-chunks)**. You can think of noun chunks as a noun plus the words describing the noun – for example, “the lavish green grass” or “the world’s largest tech fund”.\n",
    "\n",
    "    Text: The original noun chunk text.\n",
    "    Root text: The original text of the word connecting the noun chunk to the rest of the parse.\n",
    "    Root dep: Dependency relation connecting the root to its head.\n",
    "    Root head text: The text of the root token’s head.\n",
    "    Children: The immediate syntactic dependents of the root token.\n",
    "    \n",
    " - spaCy uses the terms **head** and **child** to describe the words connected by a single arc in the dependency tree. \n",
    " - The term **dep** is used for the arc label, which describes the type of syntactic relation that connects the child to the head.\n",
    " \n",
    "We can extract further relations by examining the noun modifiers in the noun chunks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['spacy_title'][0])\n",
    "\n",
    "print([x for x in df['spacy_title'][0].noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nlp(\"\"\"\\\n",
    "    Google is expanding its pool of machine learning talent with the purchase of a startup that specializes in 'instant' smartphone image recognition. \\\n",
    "    On Wednesday, French firm Moodstocks announced on its website that it's being acquired by Google, stating that it expects the deal to be completed in the next few weeks. \\\n",
    "    There's no word yet on how much Google is paying for the company. \\\n",
    "    Moodstocks' \"on-device image recognition\" software for smartphones will be phased out as it joins Google. \\\n",
    "    Moodstocks' team will also move over to Google's R&D center in Paris, according to Google's French blog. \\\n",
    "    \"Ever since we started Moodstocks, our dream has been to give eyes to machines by \\\n",
    "    turning cameras into smart sensors able to make sense of their surroundings,\" Moodstocks said in a statement on its site.\n",
    "    \"Our focus will be to build great image recognition tools within Google, \\\n",
    "    but rest assured that current paying Moodstocks customers will be able to use it until the end of their subscription.\" \n",
    "    \"\"\")\n",
    "\n",
    "words = nlp(\"Barack Obama was born in Hawaii.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1d96806e871e80c0fc0f9316d33db3714fdedfc"
   },
   "outputs": [],
   "source": [
    "dat = list()\n",
    "for chunk in df['spacy_title'][0].noun_chunks:\n",
    "    dat.append(pd.DataFrame([chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text,[c for c in chunk.root.children]]).T)\n",
    "\n",
    "print(displacy.render(df['spacy_title'][0], style='dep', jupyter=True, options={'distance':110}))\n",
    "print(displacy.render(df['spacy_title'][0], style='ent', jupyter=True, options={'distance':110}))\n",
    "\n",
    "dat = pd.concat(dat)\n",
    "dat.columns=['Chunk','root.text','root.dep','root.head','root.child']\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_chain = \"-\".join([d.tag_ for d in df['spacy_title'][0]])\n",
    "for w in words[ents[0].end:ents[1].start]:\n",
    "    ents = words.ents\n",
    "    if w.tag_ == 'VBZ':\n",
    "        n += 1\n",
    "        print(words)\n",
    "        print(pos_chain)\n",
    "        print((ents[0],ents[0].label_),\n",
    "              (w,w.tag_),\n",
    "              (ents[1],ents[1].label_),'\\n')\n",
    "    elif w.tag_ == 'VBN':\n",
    "        n += 1\n",
    "        print(words)\n",
    "        print(pos_chain)\n",
    "        print((ents[0],ents[0].label_),\n",
    "              (w,w.tag_),\n",
    "              (ents[1],ents[1].label_),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other factors to consider:\n",
    "\n",
    " - Ownership: E.g. Noun or Named Entity followed by : [NNS/VBZ](https://sites.google.com/site/partofspeechhelp/home/nns_vbz)\n",
    " - [KG and pruning](http://philipperemy.github.io/information-extract/)\n",
    "  - [git](https://github.com/philipperemy/information-extraction-with-dominating-rules)\n",
    " \n",
    "### References\n",
    "\n",
    " - [OLLIE](https://www.reddit.com/r/LanguageTechnology/comments/bovsf5/we_release_opiec_the_largest_open_information/)\n",
    " - [Clausie](https://github.com/mmxgn/clausiepy)\n",
    " - [Minie](https://github.com/mmxgn/miniepy/graphs/contributors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
